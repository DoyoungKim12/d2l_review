# 4.6. Generalization in Classification
<br>

지금까지 우리는 다중 출력과 소프트맥스 함수를 적용한 선형 신경망을 훈련시켜 다중 분류 문제를 해결하는 방법에 초점을 맞추었다. 
모델의 출력을 확률값으로 해석하여 교차 엔트로피 손실 함수를 도출했는데, 이는 (고정된 파라미터 집합에 대해) 모델이 각 라벨에 할당하는 음의 log likelihood를 계산한다. 
그리고 마지막으로, 우리의 모델을 학습용 데이터셋에 적합(fitting)시켜 이러한 도구들을 실전에 적용한다. 
그러나 항상 그렇듯이, 우리의 목표는 학습에 사용되지 않은(unseen) 데이터에서 경험적으로 평가되는 **일반적인 패턴**을 배우는 것이다. 
학습용 데이터셋에 대한 높은 정확도는 아무 의미가 없다.
각각의 입력이 고유하더라도(그리고 실제로 대부분의 고차원 데이터셋에서는 각 입력이 고유함(unique)), 
첫 번째 에폭에서 학습에 사용된 데이터셋 전체를 암기하고, 이후 입력를 볼 때마다 레이블을 조회하는 것만으로도 학습용 데이터셋에 대해서는 완벽한 정확도를 달성할 수 있다. 
그리고, 학습용 데이터셋의 정확한 라벨을 암기하는 것은 당연히 새로운 입력에 대해 분류하는 방법을 알려주지 않는다. 
그 이상의 지침이 없으면, 새로운 입력을 만날 때마다 무작위 추측에 의존해야 할 수 있다.
<br>

아래의 질문들은 지금 당장 관심을 가져야 할 것들이다: 
1. 모집단에 대한 분류기의 정확도를 정확하게 추정하기 위해 얼마나 많은 테스트 셋이 필요한가? 
2. 동일한 테스트로 모델을 반복 평가하면 어떻게 되는가? 
3. 우리의 선형 모델을 학습용 데이터셋에 적합시킨 것이 우리의 나이브한 기억 체계보다 더 잘 동작할 것이라고 기대하는 이유는 무엇인가?
<br>

Section 3.6에서는 선형 회귀의 관점에서 과적합과 일반화의 기본을 소개했지만, 이 장에서는 통계적 학습 이론의 몇 가지 기본 아이디어를 소개하면서 조금 더 깊이 있게 다룰 것이다. 
학자들은 많은 경우에 **일반화를 선험적으로 보장**할 수 있는 것을 밝혀냈다. 
많은 모델, 추구하는 일반화 갭(training error와 test error의 차이)의 상한에 대해 우리는 일반적으로 필요한 샘플 수 **n**을 결정할 수 있다. 
따라서 우리의 훈련 세트에 최소한 샘플이 포함되어 있다면, 우리의 경험적 오류는 모든 데이터 생성 분포에 대해 실제 오류 내에 있을 것이다.
ution. 
불행하게도, 이러한 종류의 보장은 깊은 지적 구성 요소를 제공하지만, 딥 러닝 실무자에게 제한된 실용적인 유용성이라는 것이 밝혀졌다. 
간단히 말해서, 이러한 보장은 심층 신경망의 일반화를 보장하기 위해 우리가 그 심층 신경망에 대해 일반적으로 훨씬 적은 예(수천 개)로 상당히 잘 일반화하는 것을 발견한 경우에도 
선험적으로 터무니없는 수의 예(아마도 수조 개 이상)가 필요하다는 것을 시사한다. 
따라서 딥 러닝 실무자는 선험적 보장을 완전히 포기하는 경우가 많은데, 
대신 과거에 유사한 문제에 대해 잘 일반화한 것을 기반으로 방법을 사용하고 경험적 평가를 통해 사후 일반화를 인증한다. 
섹션 5에 도착하면, 우리는 일반화를 다시 검토하고 심층 신경망이 실제로 일반화되는 이유를 설명하기 위해 생겨난 방대한 과학 문헌에 대한 가벼운 소개를 제공할 것이다.



