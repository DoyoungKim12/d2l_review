# 4.6. Generalization in Classification
<br>

지금까지 우리는 다중 출력과 소프트맥스 함수를 적용한 선형 신경망을 훈련시켜 다중 분류 문제를 해결하는 방법에 초점을 맞추었다. 
모델의 출력을 확률값으로 해석하여 교차 엔트로피 손실 함수를 도출했는데, 이는 (고정된 파라미터 집합에 대해) 모델이 각 라벨에 할당하는 음의 log likelihood를 계산한다. 
그리고 마지막으로, 우리의 모델을 학습용 데이터셋에 적합(fitting)시켜 이러한 도구들을 실전에 적용한다. 
그러나 항상 그렇듯이, 우리의 목표는 학습에 사용되지 않은(unseen) 데이터를 통해 경험적으로 평가되는 **일반적인 패턴**을 배우는 것이다. 
학습용 데이터셋에 대한 높은 정확도는 아무 의미가 없다.
각각의 입력이 고유하더라도(그리고 실제로 대부분의 고차원 데이터셋에서는 각 입력이 고유함(unique)), 
첫 번째 에폭에서 학습에 사용된 데이터셋 전체를 암기하고, 이후 입력를 볼 때마다 레이블을 조회하는 것만으로도 학습용 데이터셋에 대해서는 완벽한 정확도를 달성할 수 있다. 
그리고, 학습용 데이터셋의 정확한 라벨을 암기하는 것은 당연히 새로운 입력에 대해 분류하는 방법을 알려주지 않는다. 
그 이상의 지침이 없으면, 새로운 입력을 만날 때마다 무작위 추측에 의존해야 할 수 있다.
<br>

아래의 질문들은 지금 당장 관심을 가져야 할 것들이다: 
1. 모집단에 대한 분류기의 정확도를 정확하게 추정하기 위해 얼마나 많은 테스트 셋이 필요한가? 
2. 동일한 테스트로 모델을 반복 평가하면 어떻게 되는가? 
3. 우리의 선형 모델을 학습용 데이터셋에 적합시킨 것이 우리의 나이브한 기억 체계보다 더 잘 동작할 것이라고 기대하는 이유는 무엇인가?
<br>

Section 3.6에서는 선형 회귀의 관점에서 과적합과 일반화의 기본을 소개했지만, 이 장에서는 통계적 학습 이론의 몇 가지 기본 아이디어를 소개하면서 조금 더 깊이 있게 다룰 것이다. 
학자들은 많은 경우에 **일반화를 선험적으로 보장**할 수 있는 것을 밝혀냈다. 
많은 모델, 추구하는 일반화 갭 $\epsilon$ (training error와 test error의 차이)의 상한에 대해 우리는 일반적으로 필요한 샘플 수 $n$ 을 결정할 수 있다. 
따라서 우리의 훈련용 데이터셋에 최소한 n개의 샘플이 포함되어 있다면, 우리의 경험적 오류는 모든 데이터 생성 분포에 대해 실제 오류 $\epsilon$ 내에 있을 것이다.
하지만 아쉽게도 이러한 종류의 보장은 깊은 지적 구성 요소를 제공하지만, 딥러닝 실무자 입장에서는 그 실용적 유용성이 제한적이라는 것이 밝혀졌다. 
간단히 말해서, 이러한 보장은 신경망의 일반화를 보장하기 위해 
우리가 그 신경망에 대해 일반적으로 훨씬 적은 수의 샘플(수천 개 정도)로 상당히 잘 일반화하는 것을 발견한 경우에도 
선험적으로 터무니없는 수의 샘플(아마도 수조 개 이상)이 필요하다는 것을 시사한다. 
따라서 딥러닝 실무자는 선험적 보장이라는 것을 완전히 포기하는 경우가 많은데, 
대신 과거에 유사한 문제에 대해 해당 모델이 잘 일반화했던 사실을 근거로 특정 메소드를 차용하고 경험적 평가를 통해 사후 일반화를 보증한다. 
Section 5에서는 일반화의 개념을 다시 검토하고, 신경망이 현실적인 문제에서의 일반화가 가능한 이유를 설명하려고 시도한 방대한 과학 문헌에 대해 가볍게 소개할 것이다.
<br><br><br>

## 4.6.1 The Test Set

일반화 오차를 평가하기 위한 가장 전형적인 방법으로 이미 테스트 셋에 의존하기 시작했으므로, 그러한 오차 추정치의 특성에 대해 논의하는 것으로 시작해보자. 
그 파라미터 등을 어떻게 도출했는지에 대한 것은 일단 배제한 채로, 고정된 분류기 $f$ 에 초점을 맞추자. 그리고 분류기 $f$ 를 훈련시키는 데 사용되지 않은 새로운 샘플 데이터 셋 
$\mathcal{D} = {(\mathbf{x}^{(i)},y^{(i)})}_{i=1}^n$ 를 가지고 있다고 가정하자. 
$\mathcal{D}$ 에 대한 분류기 $f$ 의 **경험적 오차(empirical error)** 는 단순히 예측값 $f(\mathbf{x}^{(i)})$ 이 실제 레이블과 일치하지 않는 인스턴스의 비율로, 아래의 식으로 주어진다:
<br>

$$\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)}).$$
<br>

반면, **모집단 오차(population error)** 는 모집단(확률밀도함수 $p(x,y)$ 중 하나인 어떤 분포 $P(X,Y)$ )에서 기대할 수 있는, 우리의 분류기와 실제 라벨 값이 다른 샘플의 비율이다.
<br>

$$\epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) = \int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$
<br>

$\epsilon(f)$ 이 우리가 실제로 찾고자 하는 것이지만, 
한 사람 한 사람을 일일히 직접 측정하지 않고는 많은 인구 집단에서 평균 키를 직접 관찰할 수 없는 것처럼 우리는 그것을 직접 관찰할 수는 없다. 
우리는 오직 샘플에 근거하여 이 값을 추정할 수 있다. 
테스트셋 $\mathcal{D}$ 는 모집단을 '통계적으로' 대표하기 때문에, 우리는 경험적 오차를 모집단 오차의 통계적 추정치로 볼 수 있다. 
게다가 우리가 관심있는 모집단 오차는 (랜덤 변수 $\mathbf{1}(f(\mathbf{x}) \neq y)$ 의) 기대치이고, 경험적 오차는 표본 평균이기 때문에 
모집단 오차를 추정하는 것은 단순히 고전적인 평균 추정의 문제가 된다. (Section 2.6에서 언급함)
<br>

중심극한정리(central limit theorem)라고 불리는 확률 이론의 중요한 고전적 정리는 
평균 $\mu$ 과 표준 편차 $\sigma$ 를 가지는 임의의 분포로부터 추출된 n개의 랜덤 샘플이 있을 때, 
표본의 수가 무한대에 가까워짐에 따라 표본 평균은 모평균이 그 중심에 있고 표준편차가 $\sigma/\sqrt{n}$ 인 정규 분포에 가까워지는 경향이 있다는 것을 보장한다. 
이것은 우리에게 중요한 것을 말해준다: 
샘플의 수가 증가함에 따라, 우리의 테스트 오차(= 경험적 오차, $\epsilon_\mathcal{D}(f)$ 는 $\mathcal{O}(1/\sqrt{n})$ 의 비율로 모집단 오차 $\epsilon(f)$ 에 수렴해야 한다.
따라서, 테스트 오차를 두 배 더 정확하게 추정하기 위해서는, 우리는 4배 더 큰 테스트 셋을 수집해야 한다. 
시험 오차를 100배 더 줄이려면, 테스트 셋을 기존에 비해 1만 배나 더 수집해야 하는 것이다. 
일반적으로, 이러한 비율 $\mathcal{O}(1/\sqrt{n})$ 은 우리가 통계적으로 기대할 수 있는 최선의 것이다.
<br>

테스트 오차가 실제 모집단 오차에 수렴하는 기울기(asymptotic rate)에 대해 이처럼 어느 정도는 알게 되었기에, 우리는 이제 몇가지 중요한 디테일에 집중할 수 있다.
확률변수 $\mathbf{1}(f(\mathbf{x}) \neq y)$은 오직 0과 1 중 하나의 값만을 가질 수 있고, 따라서 이는 베르누이 확률변수
(1의 값을 가질 확률을 나타내는 파라미터(모수)로 특정되는 확률분포)라는 사실을 다시 상기해보자.
여기서 1이라는 값은 우리가 만든 분류기가 에러를 발생시켰다는 것을 의미하고, 따라서 확률변수의 파라미터는 모집단 오차율 $\epsilon(f)$ 이 된다.
베르누이 분포의 분산은 파라미터에 의해 결정되고, 그 파라미터는 여기서는 $\epsilon(f)$ 이다.( $\epsilon(f)(1-\epsilon(f))$ )
$\epsilon(f)$ 이 우리가 알 수 없는 값이기는 하지만, 1보다 클 수는 없다는 것을 안다. 
또한 분산은 모집단 오차가 0.5에 가까울 때 가장 크다는 점, 그리고 0 또는 1에 가까울 때 가장 작아진다는 점을 알 수 있는데,
이는 오차 $\epsilon(f)$ 의 추정치인 $\epsilon_\mathcal{D}(f)$ 의 'asymptotic 표준편차'가 $\sqrt{0.25/n}$ 보다 커질 수는 없다는 것을 보여준다.
(0.25는 0.5(1 - 0.5)를 계산한 값으로, 분산의 최대치)
<br>

이 '비율'은 우리가 한정된 수의 샘플을 가지고 있을 때보다는 테스트 셋이 무한대에 근접해가는 상황을 특정하고 있다는 점을 무시한다면,
이는 우리가 만약 테스트 오차를 모집단 오차에 근접하도록 해서 하나의 표준편차가가 $\pm 0.01$ 의 구간 내에 존재하길 바란다면, 
우리는 대략 2500개의 샘플을 수집해야 한다는 것을 말해준다.
만약 우리가 2개의 표준편차가 구간 안에 존재하도록 해서 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$ 될 확률이 95%가 되도록 적합시키고 싶다면,
우리는 1만개의 샘플이 필요할 것이다!
<br>

- $\sqrt{0.25/n}$ 에서 n에 2500을 대입하면 표본평균 분포의 표준편차가 0.01이 된다.
- 표준편차가 왜 2개가 되는지까지는 모르겠지만, $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$ 될 확률이 95% 라는 말은 $1.96*\sigma/\sqrt{n} = 0.01$ 이라는 말과 같다. 1.96을 대충 2로 근사해서 계산하면 저 식을 만족하는 n은 1만이라는 것을 알 수 있다. 
<br>

이는 ML의 많은 유명한 벤치마크들에서의 테스트셋 크기로 드러난다.
여러분은 0.01, 또는 그 이하의 오차율 개선으로 인해 매년 수천 개의 응용 딥러닝 논문이 출판된다는 것을 알게 되면 놀랄지도 모르겠다.
물론, 오차율이 0에 더욱 가까워질때, 0.01만큼의 개선은 중요한 문제가 될 수 있다.
<br>

지금까지의 분석에서 성가신 부분 중 하나는 샘플 크기가 무한대로 점점 커질 때 $\epsilon_\mathcal{D}$ 와 $\epsilon$ 사이의 관계가 어떻게 발전되는지, 
즉 점근적인(asymptotics) 부분에서만 무언가를 알게 해준다는 점이다.
다행스럽게도, 우리가 다루는 확률변수는 그 범위가 정해져있고(bounded), 우리는 Hoeffding 부등식을 적용하여 유효한 유한 표본 구간를 얻을 수 있다.
<br>

$$P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) < \exp\left( - 2n t^2 \right).$$
<br>

- [Hoeffding 부등식에 대한 보다 자세한 설명]([http://www.google.co.kr](https://keepmind.net/%ea%b8%b0%ea%b3%84%ed%95%99%ec%8a%b5-is-learning-feasible/))

추정치 $\epsilon_\mathcal{D}$ 와 실제 오차율 $\epsilon$ 의 차이가 0.01을 초과하지 않는다는 보증을 95%의 신뢰구간 내에서 허용할 수 있는 
가장 작은 데이터셋의 크기를 구한다면, 위의 점근 분석(asymptotic analysis)에 의해 제안된 10000개와 비교하여 **약 15000개의 샘플이 필요하다**는 것을 알 수 있다.
통계학을 더 자세히 공부해본다면 이러한 결론을 인정하는 트렌드가 일반적으로 유지되었다는 것을 알 수 있다. 
유한한 샘플을 가정할 때에도 유지되는 보장은 일반적으로 약간 더 보수적이다. 
사물의 체계에서, 이 숫자들은 그렇게 멀리 떨어져 있지 않으며, 우리가 법정에 설 수 있다는 보장은 없더라도 대략적인 수치를 제공하는 점근 분석의 일반적인 유용성을 반영한다.
<br><br><br>

## 4.6.2 Test Set Reuse

어떤 의미에서, 당신은 이제 경험적 기계학습 연구를 수행할 준비가 되었다. 
거의 모든 현실 세계의 모델들은 테스트셋 성능을 기반으로 개발 및 검증되며 이제 당신은 테스트셋의 마스터가 되었다.(?)
모든 고정된 분류기 $f$ 에 대해, 테스트 오차 $\epsilon_\mathcal{D}(f)$ 를 추정하고 그와 연관된 모집단 오차 $\epsilon(f)$ 에 대해 정확히 무엇을 말할 수 있는지(또는 없는지) 알고 있는 것이다.
<br>

이제 당신이 이러한 지식을 가지고 당신의 첫번째 모델 $f_1$ 을 학습시킬 준비가 되었다고 하자. 
분류기의 오차율 성능에 대해 어느 정도의 확신을 가져야 하는지 알고 있기 때문에, 위의 분석을 적용하여 테스트셋으로 지정할 적절한 샘플 수를 결정할 것이다. 
또한, Section 3.6의 교훈을 잊지 않고 검증 셋에서의 모든 예비분석, 하이퍼 파라미터 조정 및 심지어 여러 후보 모델 구조 중 적절한 구조 까지 선택함으로써 
테스트셋의 신성성을 확실히 보존했다고 가정해보자.
결국 당신은 모델 $f_1$  을 테스트셋으로 평가하고 모집단 오차의 불편추정량(unbiased estimate)를 연관된 신뢰구간과 함께 발표할 것이다.
<br>

여기까지는 모든 것이 별 문제 없어보인다. 
그러나, 그날 밤 당신은 새벽 3시에 새로운 모델링 접근법에 대한 훌륭한 아이디어와 함께 잠에서 깨게 된다. 
다음날, 당신은 새로운 모델의 코드를 작성하고 하이퍼 파라미터를 튜닝하며 새로운 모델 $f_2$ 가 작동할 뿐만 아니라 기존 모델보다 더 낮은 오차율을 갖게 만들었다.
그러나 이러한 발견의 기쁨은 당신이 마지막 평가를 준비하는 순간 사라진다.
테스트셋이 없었던 것이다!
<br>

원래 사용했던 테스트셋 $\mathcal{D}$ 가 아직 당신의 서버에 있을지라도, 당신은 지금 2가지의 큰 문제에 직면해있다.
먼저, 당신이 테스트셋을 수집할 때, 하나의 분류기 $f$ 를 평가한다는 가정 하에 일정 수준의 정확도를 결정했을 것이다.
그러나, 다수의 분류기 $f_1, ..., f_k$ 를 같은 테스트셋으로 평가하게 된다면, 당신은 잘못된 모험의 문제(problem of false discovery)를 반드시 고려해야 한다. 
이전에 당신은 하나의 분류기 $f$ 에 대해 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$ 일 확률이 95%이고, 따라서 추정치가 잘못되었을 확률은 5% 정도라고 확신했다.
그런데 k개의 분류기가 섞여있는 상황에서는 단 한개의 분류기조차 테스트셋 성능이 잘못되지 않았다고 보장하기는 어렵다. 
20개의 분류기가 고려대상이라면, 그 중 적어도 하나가 잘못된 스코어를 산출했을 가능성을 배제할 수 없다. 
(0.95의 20승은 0.35로, 65%의 확률로 하나 이상의 분류기 성능이 잘못 측정됨)
이 문제는 방대한 통계학 연구에도 불구하고 과학적 연구를 괴롭히는 지속적인 문제로 남아있는 **다중 가설 테스트(multiple hypothesis testing)** 와 관련있다.
<br>

만약 이것이 당신을 걱정시킬만한 충분한 무언가가 되지 못한다면, 당신이 후속 평가에서 얻은 결과를 믿을 수 없는 특별한 이유가 있다.
우리의 테스트셋 성능에 대한 분석이 분류기가 테스트셋과의 어떠한 접점도 없이 선택되었다는 가정 하에 있다는 점을 상기해보자.
그렇기에 우리는 테스트셋을 모집단에서 무작위로 추출된 것으로 볼 수 있었다.
그런데 여기서 당신은 다수의 함수를 같은 테스트셋으로 테스트할 뿐만 아니라, 후속 함수 $f_2$ 는 $f_1$ 의 테스트셋 성능을 관찰한 후에 선택된 모델이다.
모델러에게 테스트 셋의 정보가 유출되면, 통계학적 관점에서 해당 테스트셋은 진정한 의미의 테스트셋으로 기능할 수 없다.
이러한 문제는 **적응적 과적합(adaptive overfitting)** 이라 칭하며, 최근 이론가들과 통계학자들에게 중요한 관심사로 떠올랐다.
다행스럽게도, holdout set에서 모든 정보들이 유출될 수 있고 이론적인 최악의 상황은 정말 암울하긴 하지만,
이러한 분석은 너무 보수적인 접근일 수 있다.
실제로 분석을 수행할 때에는 진정한 의미의 테스트 셋을 생성하려고 노력하고, 그 테스트 셋을 최대한 덜 참조해야하며, 
신뢰구간을 특정할 때 다중 가설 테스트를 고려하고, 이러한 문제를 데이터셋이 작을 때 더 경계해야 한다. 
일련의 벤치마크 과제를 실행할 때, 각 라운드 이후에는 이전의 테스트셋을 검증 셋으로 강등할 수 있도록 여러 개의 테스트 셋을 유지하는 것이 좋다.
<br><br><br>

## 4.6.3 Statistical Learning Theory

테스트셋은 우리가 실제로 가진 모든 것이고, 이 사실은 이상하게도 불만족스럽다. 
첫째, 우리는 '진정한 의미의(true)' 테스트셋을 가지고 있는 경우가 드물다. 
우리가 데이터셋을 만든 사람이 아니라면, 아마도 다른 누군가가 우리의 '표면적인(ostensible)' 테스트 셋을 사용하여 그들의 분류기를 이미 평가했을 것이다. 
그리고 우리가 (데이터를 사용할) 첫 번째 권리를 받았을 때조차도 우리는 곧 좌절감을 느끼게 되는데, 
우리의 숫자(추정치)를 믿을 수 없다는 그 갉아먹는 느낌 없이 우리의 후속 모델 시도를 평가할 수 있기를 바란다. 
더욱이, 진정한 의미의 테스트셋 조차도 분류기가 실제로 모집단에 일반화되었는지 여부를 사후적으로만 알려줄 수 있으며, 
일반화되어야 할 선험성을 기대할 이유가 있는지 여부는 알 수 없다.
<br>

이러한 우려를 염두에 두면, 경험적(empirical) 데이터로 훈련된 모델이 처음 보는(unseen) 데이터에 일반화할 수 있는 이유와 시점를 설명하는 기본 원리를 명료히 하는 것을 
목표로 하는 기계 학습의 수학적 하위 분야인 통계 학습 이론의 매력을 볼 수 있을 것이다. 
수십 년 동안의 통계 학습 연구자들의 주요 목표 중 하나는 모델 클래스의 속성, 데이터셋의 샘플 수와 관련된 일반화 갭를 제한하는 것이었다.
<br>

학습 이론가들은 학습용 데이터셋 $\mathcal{S}$ 로 학습 및 평가된 분류기 $f_\mathcal{S}$ 의 경험적 오차 $\epsilon_\mathcal{S}(f_\mathcal{S})$ 와 
모집단에 대한 동일한 분류기의 실제 오차 $\epsilon(f_\mathcal{S})$ 사이의 차이를 제한하는 것을 목표로 한다. 
이는 방금 설명한 평가 문제와 비슷해 보일 수 있지만 큰 차이가 있다. 
이전에는 분류기가 고정되어 있었고 평가 목적으로만 데이터셋이 필요했다. 
그리고 실제로 모든 고정 분류기는 일반화한다. (이전에는 볼 수 없었던) 데이터셋에 대한 오차는 모집단 오차에 대한 편향되지 않은 추정치이다. 
그러나 분류기가 같은 데이터셋으로 훈련되고 평가될 때, 우리는 무엇을 말할 수 있는가? 학습 오차가 테스트 오차에 가깝다고 확신할 수 있는가?
<br>

우리의 학습된 분류기 $f_\mathcal{S}$ 가 미리 지정된 함수 집합 중에서 선택되어야 한다고 가정하자. 
단일 분류기의 오차를 추정하는 것은 쉽지만, 다수의 분류기를 고려하기 시작하면 일이 복잡해진다는 것을 테스트셋에 대한 우리의 논의에서 기억하자. 
어떤 (고정된) 하나의 분류기의 경험적 오차가 높은 확률로 실제 오차에 가까울지라도, 
일단 다수의 분류기 전체를 고려한다면, 우리는 분류기 집합에서 한 분류기만 심하게 잘못 추정된 오차를 받을 가능성에 대해 걱정해야 한다. 
이 걱정은 만약 분류기 집합에서 **특정 분류기 한 개만 오해의 소지가 있을 정도로 낮은 오차를 리턴한다면 
우리는 그 모델을 선택하여 모집단 오차를 엄청나게 과소평가할 수 있다는 것** 이다. 
더욱이 선형 모델의 경우, 매개변수가 연속적으로 평가되기 때문에 우리는 일반적으로 무한한 종류의 함수 중에서 모델을 선택하고 있다.
<br>

이 문제에 대한 야심찬 해결책 중 하나는 균일한 수렴을 증명하기 위한 분석 도구를 개발하는 것이다. 
즉, 높은 확률로 모든 분류기 $f\in\mathcal{F}$ 에 대한 경험적 오차율이 동시에 실제 오차율로 수렴될 것이다.
다시 말해서, 우리는 적어도 $1-\delta$ 의 확률로 (여기서의 $\delta$ 아주 작은 값)
(모든 분류기 중) 어떤 분류기의 오차도 아주 작은 값 $\alpha$ 이상으로 잘못 추정되지 않을 것이라고 말할 수 있는 이론적 원리를 추구한다.
분명히, 우리는 모든 모델 클래스  $\mathcal{F}$ 에 대해 위와 같이 선언할 수는 없다. 
항상 경험적 오차는 0을 달성하지만, 모집단에 대한 추론은 랜덤하게 생성된 결과조차 능가하지 못하는 모델을 기억하라.
<br>

어떤 의미에서는 저런 모델은 너무 유연한(= 아웃풋의 변동이 심한) 것으로 볼수도 있다. 균등분포로 수렴하는 결과값은 지지될 수 없다. 
반면에, (우리가 계속 주제로 삼았던) 고정된 분류기 또한 쓸모가 없다. 그것은 완벽하게 일반화되지만, 학습용 데이터셋과 테스트셋 그 어느 쪽에도 적합되지 않았다. 
따라서 모델 학습의 핵심적인 질문은 역사적으로 학습용 데이터셋에 더 적합되었으나 과적합(overfitting)의 위험이 있는 보다 민감한(높은 분산) 모델과, 
일반화는 잘하지만 과소적합(underfitting) 위험이 있는 보다 강건한(높은 편향) 모델 간의 trade-off 라는 프레임 안에 있었다. 
모델 학습 이론의 이러한 핵심 질문은 **모델이 이 스펙트럼을 따라 어디에 위치하는지를 정량화하고, 그와 관련된 어떤 보증을 제공하는 적절한 수학적 분석을 개발** 하는 것이었다.
<br>

일련의 중요한 논문에서, Vapnik과 Chervonenkis는 상대 주파수의 수렴에 대한 이론을 더 일반적인 종류의 함수로 확장했다.
(Vapnik and Chervonenkis, 1964, Vapnik and Chervonenkis, 1968, Vapnik and Chervonenkis, 1971, Vapnik and Chervonenkis, 1981, Vapnik and Chervonenkis, 1991, Vapnik and Chervonenkis, 1974)
이 일련의 작업이 기여한 주요한 성과 중 하나는 모델 클래스의 복잡성(유연성)을 측정하는 Vapnik-Chervonenkis(VC) 차원이다. 
게다가, 그들의 핵심 결과 중 하나는 VC 차원의 함수와 표본 수의 함수로 경험적 오치와 모집단 오차의 차이의 경계를 특정한다(bound)는 점이다.
<br>

$$P\left(R[p, f] - R_\mathrm{emp}[\mathbf{X}, \mathbf{Y}, f] < \alpha\right) \geq 1-\delta
\ \text{ for }\ \alpha \geq c \sqrt{(\mathrm{VC} - \log \delta)/n}.$$
<br>

위 식에서 $\delta > 0$ 는 해당 구간(bound)이 오염되었을 확률이다. 
$\alpha$ 는 일반화 갭의 상한(upper bound)이며, $n$ 은 데이터셋의 크기이다. 
마지막으로, $c > 0$ 는 상수로 발생가능한 손실의 크기에 따라 달라지는 값이다.
이 구간을 활용하는 한가지 방법은 원하는 $\delta$ 와 $\alpha$ 값을 넣어서 얼마나 많은 샘플을 모아야 하는지를 결정하는 것이다.
VC 차원은 어떤 임의의 (이진수) 라벨이라도 할당할 수 있는 데이터 포인트의 최대치를 정량화하고, 각 클래스에 대해 해당 라벨과 일치하는 결과를 내는 일부 모델을 찾는다.
예를 들어, d 차원의 입력을 받는 선형모델은 d+1 차원의 VC 차원을 가진다.
이는 3개의 점이 주어졌을 때, 각 점이 -1과 1 중 어떤 라벨을 가지든 하나의 직선으로 분리하는 것이 가능하지만,
4개의 점이 주어졌을 때에는 하나의 직선으로는 올바른 분류기를 만들 수 없다는 점을 보면 명확하다.
(이는 break point에 대한 설명으로, Break Point k의 정의는 데이터를 흩뿌릴 수 없는, 다시말해 어떻게 데이터를 배치해도 Growth Function이 $2^k$ 보다 
작은 k 중 가장 작은 값을 의미한다. VC Dimension은 이와 유사하게, 최대로 데이터를 흩뿌릴 수 있는 k를 의미한다. 즉, Break Point – 1 = VC Dimension 이다.)
(Growth Function은 n개의 Data Point가 있을 때 나올 수 있는 최대의 Dichotomy의 수를 의미하고, 
Dichotomy는 영어 사전엔 양분, 이분이란 단어로 나오는데 이는 주어진 데이터를 -1 또는 +1로 분류하는 룰을 의미한다.)
(이와 같은 방법으로 모든 vc 차원(d+1)에 대해 적당한 샘플 수 n을 구해야 하는데, 일반적으로 정확하게 구하지 않고 대략적인 값(Rule of Thumb)을 구한다고 합니다. 
이유는 데이터는 어차피 필요한 최소의 개수만 구하고 그만두지 않고 많으면 많을수록 좋기 때문이라고 생각합니다. 
대략적으로 필요한 양은 VC Dimension의 10배 이상이라고 합니다.)
<br>

불행하게도 이 이론은 복잡도가 높은 모델에 대해 지나치게 비관적인 경향이 있어, 이 보증을 얻으려면 일반적으로 원하는 오차율을 달성하는 데에 
실제로 필요한 것보다 훨씬 많은 샘플이 필요하다.
또한 모델 클래스와 $\delta$ 를 수정하면 오차율은 다시 일반적인 $\mathcal{O}(1/\sqrt{n})$ 의 비율로 감소한다.
우리가 샘플 수 $n$ 의 관점에서 개선하기는 어려워 보인다.
그러나 모델 클래스를 다양화함에 따라, VC 차원은 일반화 갭에 대한 비관적인 관점의 예측을 제시할 수 있다.
(여기는 아무리 여러번 읽어봐도 해석이 어렵네요;;)
<br><br><br>

## 4.6.4 Summery

모델을 평가하는 가장 간단한 방법은 이전에 볼 수 없었던 데이터로 구성된 테스트셋을 참조하는 것이다.
테스트셋 평가는 실제 오차에 대한 편향되지 않은 추정치를 제공하며, 테스트셋이 커짐에 따라 $\mathcal{O}(1/\sqrt{n})$ 의 비율로 수렴한다. 
우리는 정확한 점근 분포에 기반한 대략적인 신뢰구간, 또는 유한한 샘플 보장에 기반한 유효한 표본 신뢰구간 근사치를 제공할 수 있다. 
실제로 테스트셋 평가는 현대 기계학습 연구의 중요한 기반이다. 
그러나 테스트셋이 진정한 의미의 테스트셋으로 기능하는 경우가 거의 없다. (여러 연구자가 반복적으로 사용)
동일한 테스트셋을 사용하여 여러 모델을 평가하면 잘못된 발견을 제어하기가 어려울 수 있다. 
이것은 이론적으로 큰 문제를 일으킬 수 있다. 
현실에서 이 문제의 중요성은 테스트셋의 크기, 그리고 테스트셋이 단지 하이퍼파라미터를 선택하기 위해 사용되는지 아니면 더 직접적으로 정보를 유출하는지에 따라 달라진다. 
그럼에도 어쨌든 실전에서는 테스트셋을 큐레이션하고, 그 사용 빈도에 대해 가능한 한 보수적인 태도를 취하는 것은 좋은 시도이다.
<br>

보다 만족스러운 해결책을 제시하기 위해, 통계 학습 이론가들은 모델 클래스에 대해 균일한 수렴(uniform convergence)을 증명하는 방법을 개발했다. 
실제로 모든 모델의 경험적 오차가 동시에 실제 오차로 수렴된다면,  
보유한 테스트셋에서도 비슷하게 잘 수행된다는 것을 알고 훈련 오류를 최소화하면서 가장 나은 퍼포먼스를 보이는 모델을 자유롭게 선택할 수 있다. 
결정적으로, 그러한 결과는 모델 클래스의 일부 속성에 의존해야 한다. 
Vladimir Vapnik과 Alexey Chernovenkis는 VC 차원을 도입하여 VC 클래스의 모든 모델에 대해 일관된 수렴 결과를 제시했다. 
클래스의 모든 모델에 대한 학습 오차는 (동시에) 실제 오차에 가까워지고 $\mathcal{O}(1/\sqrt{n})$ 의 비율로 감소할 것을 보장한다. 
VC 차원의 혁명적인 발견에 따라, 다양한 대안적인 복잡도 측정법이 제안되었으며, 각각은 유사한 일반화 보장을 용이하게 한다. 
(함수의 복잡도 측정의 몇 가지 고도화된 방법론에 대한 자세한 논의는Boucheron 등을 참조(2005))
불행하게도, 이러한 복잡도 측정은 통계 이론에서 광범위하게 유용한 도구가 되었지만, 심층 신경망이 일반화되는 이유를 설명하는 데에는 쓸모가 없는 것으로 드러났다. 
심층 신경망은 종종 수백만, 또는 그 이상의 파라미터를 가지고 있으며, 무작위 라벨을 대규모 데이터에 쉽게 할당할 수 있다. 
그럼에도 불구하고, 그들은 현실의 문제에 대해 잘 일반화하고, 놀랍게도 더 큰 VC 차원이 발생함에도 불구하고 모델이 더 크고 깊을 때 더 잘 일반화하는 경우가 많다. 
다음 챕터에서는 딥 러닝의 맥락에서 일반화를 다시 살펴볼 것이다.
<br>









